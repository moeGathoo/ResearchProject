{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn.preprocessing as skpp\n",
    "import scipy.stats as stats\n",
    "from scipy.special import boxcox1p\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization \n",
    "from keras.layers import Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint #we can control our model if going well during validation part or not\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf = pd.read_excel(\"data.xlsx\", dtype=object)\n",
    "columnNames = dataDf.columns\n",
    "\n",
    "dataDf.dropna(inplace=True)\n",
    "dataDf['GENDER'] = dataDf['GENDER'].replace([\"Male\", \"Female\"], [1,2])\n",
    "for i in range(4, 43-9):\n",
    "    dataDf = dataDf.astype({dataDf.columns[i]: str})\n",
    "\n",
    "for i in range(len(dataDf)):\n",
    "    row = dataDf.iloc[i]\n",
    "    for c in range(4, 18):\n",
    "        row[columnNames[c]] = row[columnNames[c]].strip(\".\")\n",
    "        row[columnNames[c]] = row[columnNames[c]].split('.')\n",
    "    dataDf.iloc[i] = row\n",
    "for i in range(len(dataDf)):\n",
    "    for c in range(4, 18):\n",
    "        dataDf = dataDf.explode(columnNames[c])\n",
    "\n",
    "dropList = ['Questioinnaire number ', 'AGE', 'HOME LANGUAGE']\n",
    "dataDf = dataDf.drop(columns=dropList)\n",
    "\n",
    "dataDf['PASSED'] = 0\n",
    "dataDf.loc[dataDf['FNL ILS'] >= 50, 'PASSED'] = 1\n",
    "\n",
    "columns = dataDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf = dataDf.drop(columns=columns[31:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = dataDf.columns\n",
    "dataDf = dataDf.drop_duplicates()\n",
    "dataDf = dataDf.dropna()\n",
    "i = 0\n",
    "for c in dataDf.columns:\n",
    "    dataDf[c] = pd.to_numeric(dataDf[c])\n",
    "    dataDf[c] = dataDf[c].astype(int)\n",
    "    feature = \"feature\" + str(i)\n",
    "    dataDf = dataDf.rename(columns={c:feature})\n",
    "    i = i + 1\n",
    "columns = dataDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPor = dataDf.iloc[:, -1]\n",
    "from sklearn.model_selection import train_test_split\n",
    "xTrainDf, xTestDf, yTrainDf, yTestDf = train_test_split(dataDf, yPor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature0Vocab = np.arange(dataDf[columns[0]].min(), dataDf[columns[0]].max()+1)\n",
    "feature0Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[0], vocabulary_list=feature0Vocab)\n",
    "\n",
    "feature1Vocab = np.arange(dataDf[columns[1]].min(), dataDf[columns[1]].max()+1)\n",
    "feature1Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[1], vocabulary_list=feature1Vocab)\n",
    "\n",
    "feature2Vocab = np.arange(dataDf[columns[2]].min(), dataDf[columns[2]].max()+1)\n",
    "feature2Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[2], vocabulary_list=feature2Vocab)\n",
    "\n",
    "feature3Vocab = np.arange(dataDf[columns[3]].min(), dataDf[columns[3]].max()+1)\n",
    "feature3Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[3], vocabulary_list=feature3Vocab)\n",
    "\n",
    "feature4Vocab = np.arange(dataDf[columns[4]].min(), dataDf[columns[4]].max()+1)\n",
    "feature4Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[4], vocabulary_list=feature4Vocab)\n",
    "\n",
    "feature5Vocab = np.arange(dataDf[columns[5]].min(), dataDf[columns[5]].max()+1)\n",
    "feature5Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[5], vocabulary_list=feature5Vocab)\n",
    "\n",
    "feature6Vocab = np.arange(dataDf[columns[6]].min(), dataDf[columns[6]].max()+1)\n",
    "feature6Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[6], vocabulary_list=feature6Vocab)\n",
    "\n",
    "feature7Vocab = np.arange(dataDf[columns[7]].min(), dataDf[columns[7]].max()+1)\n",
    "feature7Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[7], vocabulary_list=feature7Vocab)\n",
    "\n",
    "feature8Vocab = np.arange(dataDf[columns[8]].min(), dataDf[columns[8]].max()+1)\n",
    "feature8Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[8], vocabulary_list=feature8Vocab)\n",
    "\n",
    "feature9Vocab = np.arange(dataDf[columns[9]].min(), dataDf[columns[9]].max()+1)\n",
    "feature9Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[9], vocabulary_list=feature9Vocab)\n",
    "\n",
    "feature10Vocab = np.arange(dataDf[columns[10]].min(), dataDf[columns[10]].max()+1)\n",
    "feature10Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[10], vocabulary_list=feature10Vocab)\n",
    "\n",
    "feature11Vocab = np.arange(dataDf[columns[11]].min(), dataDf[columns[11]].max()+1)\n",
    "feature11Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[11], vocabulary_list=feature11Vocab)\n",
    "\n",
    "feature12Vocab = np.arange(dataDf[columns[12]].min(), dataDf[columns[12]].max()+1)\n",
    "feature12Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[12], vocabulary_list=feature12Vocab)\n",
    "\n",
    "feature13Vocab = np.arange(dataDf[columns[13]].min(), dataDf[columns[13]].max()+1)\n",
    "feature13Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[13], vocabulary_list=feature13Vocab)\n",
    "\n",
    "feature14Vocab = np.arange(dataDf[columns[14]].min(), dataDf[columns[14]].max()+1)\n",
    "feature14Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[14], vocabulary_list=feature14Vocab)\n",
    "\n",
    "feature15Vocab = np.arange(dataDf[columns[15]].min(), dataDf[columns[15]].max()+1)\n",
    "feature15Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[15], vocabulary_list=feature15Vocab)\n",
    "\n",
    "feature16Vocab = np.arange(dataDf[columns[16]].min(), dataDf[columns[16]].max()+1)\n",
    "feature16Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[16], vocabulary_list=feature16Vocab)\n",
    "\n",
    "feature17Vocab = np.arange(dataDf[columns[17]].min(), dataDf[columns[17]].max()+1)\n",
    "feature17Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[17], vocabulary_list=feature17Vocab)\n",
    "\n",
    "feature18Vocab = np.arange(dataDf[columns[18]].min(), dataDf[columns[18]].max()+1)\n",
    "feature18Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[18], vocabulary_list=feature18Vocab)\n",
    "\n",
    "feature19Vocab = np.arange(dataDf[columns[19]].min(), dataDf[columns[19]].max()+1)\n",
    "feature19Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[19], vocabulary_list=feature19Vocab)\n",
    "\n",
    "feature20Vocab = np.arange(dataDf[columns[20]].min(), dataDf[columns[20]].max()+1)\n",
    "feature20Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[20], vocabulary_list=feature20Vocab)\n",
    "\n",
    "feature21Vocab = np.arange(dataDf[columns[21]].min(), dataDf[columns[21]].max()+1)\n",
    "feature21Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[21], vocabulary_list=feature21Vocab)\n",
    "\n",
    "feature22Vocab = np.arange(dataDf[columns[22]].min(), dataDf[columns[22]].max()+1)\n",
    "feature22Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[22], vocabulary_list=feature22Vocab)\n",
    "\n",
    "feature23Vocab = np.arange(dataDf[columns[23]].min(), dataDf[columns[23]].max()+1)\n",
    "feature23Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[23], vocabulary_list=feature23Vocab)\n",
    "\n",
    "feature24Vocab = np.arange(dataDf[columns[24]].min(), dataDf[columns[24]].max()+1)\n",
    "feature24Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[24], vocabulary_list=feature24Vocab)\n",
    "\n",
    "feature25Vocab = np.arange(dataDf[columns[25]].min(), dataDf[columns[25]].max()+1)\n",
    "feature25Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[25], vocabulary_list=feature25Vocab)\n",
    "\n",
    "feature26Vocab = np.arange(dataDf[columns[26]].min(), dataDf[columns[26]].max()+1)\n",
    "feature26Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[26], vocabulary_list=feature26Vocab)\n",
    "\n",
    "feature27Vocab = np.arange(dataDf[columns[27]].min(), dataDf[columns[27]].max()+1)\n",
    "feature27Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[27], vocabulary_list=feature27Vocab)\n",
    "\n",
    "feature28Vocab = np.arange(dataDf[columns[28]].min(), dataDf[columns[28]].max()+1)\n",
    "feature28Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[28], vocabulary_list=feature28Vocab)\n",
    "\n",
    "feature29Vocab = np.arange(dataDf[columns[29]].min(), dataDf[columns[29]].max()+1)\n",
    "feature29Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[29], vocabulary_list=feature29Vocab)\n",
    "\n",
    "feature30Vocab = np.arange(dataDf[columns[30]].min(), dataDf[columns[30]].max()+1)\n",
    "feature30Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[30], vocabulary_list=feature30Vocab)\n",
    "\n",
    "feature31Vocab = np.arange(dataDf[columns[31]].min(), dataDf[columns[31]].max()+1)\n",
    "feature31Column = tf.feature_column.categorical_column_with_vocabulary_list(key=columns[31], vocabulary_list=feature31Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureColumns = [\n",
    "    tf.feature_column.indicator_column(feature0Column),\n",
    "    tf.feature_column.indicator_column(feature1Column),\n",
    "    tf.feature_column.indicator_column(feature2Column),\n",
    "    tf.feature_column.indicator_column(feature3Column),\n",
    "    tf.feature_column.indicator_column(feature4Column),\n",
    "    tf.feature_column.indicator_column(feature5Column),\n",
    "    tf.feature_column.indicator_column(feature6Column),\n",
    "    tf.feature_column.indicator_column(feature7Column),\n",
    "    tf.feature_column.indicator_column(feature8Column),\n",
    "    tf.feature_column.indicator_column(feature9Column),\n",
    "    tf.feature_column.indicator_column(feature10Column),\n",
    "    tf.feature_column.indicator_column(feature11Column),\n",
    "    tf.feature_column.indicator_column(feature12Column),\n",
    "    tf.feature_column.indicator_column(feature13Column),\n",
    "    tf.feature_column.indicator_column(feature14Column),\n",
    "    tf.feature_column.indicator_column(feature15Column),\n",
    "    tf.feature_column.indicator_column(feature16Column),\n",
    "    tf.feature_column.indicator_column(feature17Column),\n",
    "    tf.feature_column.indicator_column(feature18Column),\n",
    "    tf.feature_column.indicator_column(feature19Column),\n",
    "    tf.feature_column.indicator_column(feature20Column),\n",
    "    tf.feature_column.indicator_column(feature21Column),\n",
    "    tf.feature_column.indicator_column(feature22Column),\n",
    "    tf.feature_column.indicator_column(feature23Column),\n",
    "    tf.feature_column.indicator_column(feature24Column),\n",
    "    tf.feature_column.indicator_column(feature25Column),\n",
    "    tf.feature_column.indicator_column(feature26Column),\n",
    "    tf.feature_column.indicator_column(feature27Column),\n",
    "    tf.feature_column.indicator_column(feature28Column),\n",
    "    tf.feature_column.indicator_column(feature29Column),\n",
    "    tf.feature_column.indicator_column(feature30Column),\n",
    "    tf.feature_column.indicator_column(feature31Column)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\migat\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\lazy_loader.py:59: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\migat\\AppData\\Local\\Temp\\ipykernel_13716\\452900260.py:1: The name tf.estimator.inputs.pandas_input_fn is deprecated. Please use tf.compat.v1.estimator.inputs.pandas_input_fn instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputFunction = tf.compat.v1.estimator.inputs.pandas_input_fn(x=xTrainDf,y=yTrainDf ,batch_size=100,num_epochs=10,\n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\migat\\AppData\\Local\\Temp\\tmp7nbfxpxn\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\migat\\\\AppData\\\\Local\\\\Temp\\\\tmp7nbfxpxn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.DNNClassifier(hidden_units=[24,24,24], feature_columns=featureColumns,\n",
    "                                  optimizer=tf.optimizers.Adam(learning_rate=0.01),\n",
    "                                  activation_fn=tf.nn.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\migat\\AppData\\Local\\Temp\\tmpkls4r5j_\\model.ckpt.\n",
      "INFO:tensorflow:loss = 17238.445, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 49 into C:\\Users\\migat\\AppData\\Local\\Temp\\tmpkls4r5j_\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 419.46112.\n"
     ]
    }
   ],
   "source": [
    "model.train(input_fn=inputFunction,steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predInputFunc = tf.estimator.inputs.pandas_input_fn(\n",
    "      x=xTestDf,\n",
    "      batch_size=100,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predGen = model.predict(predInputFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\migat\\AppData\\Local\\Temp\\tmpkls4r5j_\\model.ckpt-49\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predictions = list(predGen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f523ce2d606dc9235b909666820dd35e4e5fcce9a0c3454d0af14d6b35f2fdea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
